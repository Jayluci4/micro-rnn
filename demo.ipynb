{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# micro-rnn: Understanding RNNs from Scratch\n",
        "\n",
        "This notebook walks through the implementation of a vanilla RNN for character-level language modeling.\n",
        "\n",
        "**What you'll learn:**\n",
        "1. How RNNs process sequential data\n",
        "2. Forward pass through time\n",
        "3. Backpropagation Through Time (BPTT)\n",
        "4. The vanishing gradient problem\n",
        "5. Why LSTMs/GRUs were invented\n",
        "\n",
        "**Prerequisites:**\n",
        "- Karpathy's micrograd (backprop, neural networks)\n",
        "- Basic Python and NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)  # For reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The Problem: Sequential Data\n",
        "\n",
        "Regular neural networks (MLPs) take fixed-size inputs. But what about:\n",
        "- Text (variable length sentences)\n",
        "- Audio (time series)\n",
        "- Video (sequence of frames)\n",
        "\n",
        "We need a network that can process **sequences** and maintain **memory** of what it has seen.\n",
        "\n",
        "Enter the **Recurrent Neural Network (RNN)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. RNN Architecture\n",
        "\n",
        "The key idea: **hidden state** that gets updated at each timestep.\n",
        "\n",
        "```\n",
        "h_t = tanh(W_xh @ x_t + W_hh @ h_{t-1} + b_h)\n",
        "y_t = W_hy @ h_t + b_y\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `x_t` = input at time t\n",
        "- `h_t` = hidden state at time t (the \"memory\")\n",
        "- `y_t` = output at time t\n",
        "- `W_hh` = **recurrent weight** (this is what makes it an RNN!)\n",
        "\n",
        "The hidden state `h_t` carries information from all previous timesteps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Let's Build It!\n",
        "\n",
        "First, let's implement the core RNN cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleRNN:\n",
        "    \"\"\"\n",
        "    Minimal RNN for character-level language modeling.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize weights with small random values\n",
        "        self.W_xh = np.random.randn(hidden_size, input_size) * 0.01   # input -> hidden\n",
        "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden -> hidden (RECURRENT!)\n",
        "        self.W_hy = np.random.randn(output_size, hidden_size) * 0.01  # hidden -> output\n",
        "        \n",
        "        # Biases\n",
        "        self.b_h = np.zeros((hidden_size, 1))\n",
        "        self.b_y = np.zeros((output_size, 1))\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "    \n",
        "    def forward_step(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        Single timestep forward pass.\n",
        "        \n",
        "        This is the heart of the RNN - it combines:\n",
        "        - Current input (x)\n",
        "        - Previous memory (h_prev)\n",
        "        To produce new memory (h) and output (y)\n",
        "        \"\"\"\n",
        "        # New hidden state = tanh(input contribution + memory contribution)\n",
        "        h = np.tanh(\n",
        "            np.dot(self.W_xh, x) +      # What the current input says\n",
        "            np.dot(self.W_hh, h_prev) +  # What we remember from before\n",
        "            self.b_h\n",
        "        )\n",
        "        \n",
        "        # Output logits\n",
        "        y = np.dot(self.W_hy, h) + self.b_y\n",
        "        \n",
        "        # Softmax for probabilities\n",
        "        p = np.exp(y - np.max(y))\n",
        "        p = p / np.sum(p)\n",
        "        \n",
        "        return h, y, p\n",
        "\n",
        "# Create a small RNN\n",
        "vocab_size = 5\n",
        "hidden_size = 3\n",
        "rnn = SimpleRNN(vocab_size, hidden_size, vocab_size)\n",
        "\n",
        "print(f\"Created RNN with:\")\n",
        "print(f\"  - Input size: {vocab_size}\")\n",
        "print(f\"  - Hidden size: {hidden_size}\")\n",
        "print(f\"  - Output size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Forward Pass Through Time\n",
        "\n",
        "Let's see how the hidden state evolves as we feed in a sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sequence of 4 one-hot encoded inputs\n",
        "sequence = [0, 2, 1, 3]  # character indices\n",
        "\n",
        "# Initialize hidden state to zeros\n",
        "h = np.zeros((hidden_size, 1))\n",
        "print(f\"Initial hidden state: {h.flatten()}\")\n",
        "print(\"\\nProcessing sequence...\\n\")\n",
        "\n",
        "hidden_states = [h.flatten()]\n",
        "\n",
        "for t, idx in enumerate(sequence):\n",
        "    # One-hot encode input\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[idx] = 1\n",
        "    \n",
        "    # Forward step\n",
        "    h, y, p = rnn.forward_step(x, h)\n",
        "    hidden_states.append(h.flatten())\n",
        "    \n",
        "    print(f\"t={t}: input={idx}, hidden={h.flatten().round(3)}, top_pred={np.argmax(p)}\")\n",
        "\n",
        "# Visualize hidden state evolution\n",
        "hidden_states = np.array(hidden_states)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(hidden_size):\n",
        "    plt.plot(hidden_states[:, i], marker='o', label=f'h[{i}]')\n",
        "plt.xlabel('Timestep')\n",
        "plt.ylabel('Activation')\n",
        "plt.title('Hidden State Evolution Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNotice how each hidden unit changes as it processes the sequence!\")\n",
        "print(\"This is the RNN's 'memory' being updated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Character-Level Language Modeling\n",
        "\n",
        "Our task: given a sequence of characters, predict the next character.\n",
        "\n",
        "```\n",
        "Input:  h e l l\n",
        "Target: e l l o\n",
        "```\n",
        "\n",
        "Let's set up a real dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our training data\n",
        "text = \"hello world hello rnn hello deep learning\"\n",
        "\n",
        "# Create vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(f\"Text: '{text}'\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Characters: {chars}\")\n",
        "\n",
        "# Convert text to indices\n",
        "data = [char_to_idx[ch] for ch in text]\n",
        "print(f\"\\nAs indices: {data[:20]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. The Backward Pass: BPTT\n",
        "\n",
        "Now the tricky part: **Backpropagation Through Time (BPTT)**.\n",
        "\n",
        "The key insight: gradients flow backward through the `W_hh` connections.\n",
        "\n",
        "At each timestep, gradient gets multiplied by `W_hh`. If we have T timesteps:\n",
        "- Gradient ~ `(W_hh)^T`\n",
        "\n",
        "This causes problems:\n",
        "- If eigenvalues of W_hh < 1: gradient **vanishes** (shrinks exponentially)\n",
        "- If eigenvalues of W_hh > 1: gradient **explodes** (grows exponentially)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see the vanishing gradient in action!\n",
        "from rnn import RNN, create_dataset, get_batch\n",
        "\n",
        "# Create RNN\n",
        "char_to_idx, idx_to_char, data = create_dataset(text)\n",
        "vocab_size = len(char_to_idx)\n",
        "hidden_size = 50\n",
        "rnn = RNN(vocab_size, hidden_size, vocab_size)\n",
        "\n",
        "# Test with different sequence lengths\n",
        "seq_lengths = [5, 10, 20, 30]\n",
        "\n",
        "print(\"Gradient magnitude at first timestep vs last timestep:\\n\")\n",
        "\n",
        "for seq_len in seq_lengths:\n",
        "    # Create dummy data\n",
        "    inputs = list(np.random.randint(0, vocab_size, seq_len))\n",
        "    targets = list(np.random.randint(0, vocab_size, seq_len))\n",
        "    \n",
        "    h_prev = np.zeros((hidden_size, 1))\n",
        "    loss, cache, _ = rnn.forward(inputs, targets, h_prev)\n",
        "    \n",
        "    xs, hs, ps = cache\n",
        "    \n",
        "    # Track gradient at each timestep\n",
        "    dh_next = np.zeros((hidden_size, 1))\n",
        "    grad_mags = []\n",
        "    \n",
        "    for t in reversed(range(seq_len)):\n",
        "        dy = np.copy(ps[t])\n",
        "        dy[targets[t]] -= 1\n",
        "        dh = np.dot(rnn.W_hy.T, dy) + dh_next\n",
        "        dh_raw = (1 - hs[t] ** 2) * dh\n",
        "        grad_mags.append(np.linalg.norm(dh_raw))\n",
        "        dh_next = np.dot(rnn.W_hh.T, dh_raw)\n",
        "    \n",
        "    grad_mags = grad_mags[::-1]\n",
        "    ratio = grad_mags[-1] / (grad_mags[0] + 1e-10)\n",
        "    \n",
        "    print(f\"Seq length {seq_len:2d}: first={grad_mags[0]:.4f}, last={grad_mags[-1]:.4f}, ratio={ratio:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizing the Vanishing Gradient\n",
        "\n",
        "Let's plot how gradient magnitude changes across timesteps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Longer sequence to see the effect clearly\n",
        "seq_len = 50\n",
        "inputs = list(np.random.randint(0, vocab_size, seq_len))\n",
        "targets = list(np.random.randint(0, vocab_size, seq_len))\n",
        "\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "loss, cache, _ = rnn.forward(inputs, targets, h_prev)\n",
        "xs, hs, ps = cache\n",
        "\n",
        "# Backward pass tracking gradients\n",
        "dh_next = np.zeros((hidden_size, 1))\n",
        "grad_mags = []\n",
        "\n",
        "for t in reversed(range(seq_len)):\n",
        "    dy = np.copy(ps[t])\n",
        "    dy[targets[t]] -= 1\n",
        "    dh = np.dot(rnn.W_hy.T, dy) + dh_next\n",
        "    dh_raw = (1 - hs[t] ** 2) * dh\n",
        "    grad_mags.append(np.linalg.norm(dh_raw))\n",
        "    dh_next = np.dot(rnn.W_hh.T, dh_raw)\n",
        "\n",
        "grad_mags = grad_mags[::-1]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(grad_mags, 'b-', linewidth=2)\n",
        "plt.xlabel('Timestep')\n",
        "plt.ylabel('Gradient Magnitude')\n",
        "plt.title('Gradient Flow Through Time')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.semilogy(grad_mags, 'r-', linewidth=2)\n",
        "plt.xlabel('Timestep')\n",
        "plt.ylabel('Gradient Magnitude (log scale)')\n",
        "plt.title('Gradient Flow (Log Scale)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey insight: Gradients for early timesteps are MUCH smaller!\")\n",
        "print(\"This means the RNN struggles to learn long-range dependencies.\")\n",
        "print(\"\\nSolution: LSTM and GRU use 'gates' to control gradient flow.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training the RNN\n",
        "\n",
        "Now let's train our RNN to generate text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "hidden_size = 64\n",
        "seq_length = 15\n",
        "learning_rate = 0.1\n",
        "num_iterations = 3000\n",
        "\n",
        "# Fresh RNN\n",
        "rnn = RNN(vocab_size, hidden_size, vocab_size)\n",
        "\n",
        "# Training loop\n",
        "losses = []\n",
        "smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
        "\n",
        "n = 0\n",
        "p = 0\n",
        "h_prev = np.zeros((hidden_size, 1))\n",
        "\n",
        "print(\"Training...\\n\")\n",
        "\n",
        "for n in range(num_iterations):\n",
        "    # Reset if at end of data\n",
        "    if p + seq_length + 1 >= len(data):\n",
        "        p = 0\n",
        "        h_prev = np.zeros((hidden_size, 1))\n",
        "    \n",
        "    # Get batch\n",
        "    inputs, targets = get_batch(data, p, seq_length)\n",
        "    \n",
        "    # Forward\n",
        "    loss, cache, h_prev = rnn.forward(inputs, targets, h_prev)\n",
        "    \n",
        "    # Backward\n",
        "    grads = rnn.backward(inputs, targets, cache)\n",
        "    grads = rnn.clip_gradients(grads)\n",
        "    \n",
        "    # Update\n",
        "    rnn.update_parameters(grads, learning_rate)\n",
        "    \n",
        "    # Track loss\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "    losses.append(smooth_loss)\n",
        "    \n",
        "    if n % 500 == 0:\n",
        "        print(f\"Iteration {n}: loss = {smooth_loss:.4f}\")\n",
        "    \n",
        "    p += seq_length\n",
        "\n",
        "print(f\"\\nFinal loss: {smooth_loss:.4f}\")\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Generate Text!\n",
        "\n",
        "Let's see what our trained RNN can produce:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate text\n",
        "def generate_text(rnn, seed_char, length):\n",
        "    h = np.zeros((rnn.hidden_size, 1))\n",
        "    seed_idx = char_to_idx[seed_char]\n",
        "    indices = rnn.sample(seed_idx, h, length)\n",
        "    return seed_char + ''.join(idx_to_char[i] for i in indices)\n",
        "\n",
        "print(\"Generated text samples:\\n\")\n",
        "\n",
        "for seed in ['h', 'w', 'd']:\n",
        "    generated = generate_text(rnn, seed, 50)\n",
        "    print(f\"Seed '{seed}': {generated}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Key Takeaways\n",
        "\n",
        "### What we learned:\n",
        "\n",
        "1. **RNN Architecture**: Hidden state carries memory through time via W_hh\n",
        "\n",
        "2. **Forward Pass**: h_t = tanh(W_xh @ x_t + W_hh @ h_{t-1} + b_h)\n",
        "\n",
        "3. **BPTT**: Gradients flow backward through W_hh connections\n",
        "\n",
        "4. **Vanishing Gradient**: Gradients shrink exponentially for long sequences\n",
        "\n",
        "5. **Gradient Clipping**: Simple fix for exploding gradients\n",
        "\n",
        "### Why this matters:\n",
        "\n",
        "The vanishing gradient problem is why **LSTMs and GRUs** were invented!\n",
        "\n",
        "They use **gates** to control gradient flow:\n",
        "- Forget gate: what to forget from memory\n",
        "- Input gate: what new info to add\n",
        "- Output gate: what to output\n",
        "\n",
        "This allows gradients to flow unchanged through many timesteps.\n",
        "\n",
        "### Next steps:\n",
        "\n",
        "1. **micro-lstm**: Implement LSTM from scratch to see how gates work\n",
        "2. **micro-gru**: Simplified version with fewer gates\n",
        "3. **micro-seq2seq**: Encoder-decoder architecture for translation\n",
        "\n",
        "Then you'll be ready for **attention** and **transformers**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Congratulations! You now understand RNNs from scratch!\")\n",
        "print(\"\\nKey formula to remember:\")\n",
        "print(\"  h_t = tanh(W_xh @ x_t + W_hh @ h_{t-1} + b_h)\")\n",
        "print(\"\\nThe W_hh is what makes it 'recurrent' - and causes vanishing gradients!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
